{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJptKBxALl-u",
    "outputId": "a4bddfbd-78c1-47e5-8b6a-c2aaf654279f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.78 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00Owi1LBNY8L",
    "outputId": "9b59ed1f-019f-402a-98c4-337babe98c28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQZaZRGcNLtr",
    "outputId": "dc8ff540-bbc8-4224-f335-a6a41648ab95"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1875, 313)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3gEjf-xMb-N"
   },
   "source": [
    "# Some Notes on our naive model\n",
    "\n",
    "We are going to write a network based on what we have learnt so far.\n",
    "\n",
    "The size of the input image is 28x28x1. We are going to add as many layers as required to reach RF = 32 \"atleast\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sir2LmSVLr_4"
   },
   "outputs": [],
   "source": [
    "class FirstDNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(FirstDNN, self).__init__()\n",
    "    # r_in:1, n_in:28, j_in:1, s:1, r_out:3, n_out:28, j_out:1\n",
    "    self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
    "    self.pool1 = nn.MaxPool2d(2, 2)\n",
    "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
    "    self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
    "    self.conv4 = nn.Conv2d(128, 256, 3, padding = 1)\n",
    "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
    "    self.pool2 = nn.MaxPool2d(2, 2)\n",
    "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
    "    self.conv5 = nn.Conv2d(256, 512, 3)\n",
    "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
    "    self.conv6 = nn.Conv2d(512, 1024, 3)\n",
    "    # r_in: , n_in: , j_in: , s: , r_out: , n_out: , j_out:\n",
    "    self.conv7 = nn.Conv2d(1024, 10, 3)\n",
    "# Correct values\n",
    "# https://user-images.githubusercontent.com/498461/238034116-7db4cec0-7738-42df-8b67-afa971428d39.png\n",
    "  def forward(self, x):\n",
    "    x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
    "    x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
    "    x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
    "    x = F.relu(self.conv7(x)) #<< Correct answer is to remove the ReLU from here\n",
    "    x = x.view(-1, 10)\n",
    "    return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxICO4TTNt2H"
   },
   "outputs": [],
   "source": [
    "model = FirstDNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M__MtFIYNwXa",
    "outputId": "e9c6eaa9-e672-4966-9300-40b7a642d724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             320\n",
      "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
      "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
      "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
      "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
      "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
      "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
      "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
      "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
      "================================================================\n",
      "Total params: 6,379,786\n",
      "Trainable params: 6,379,786\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.51\n",
      "Params size (MB): 24.34\n",
      "Estimated Total Size (MB): 25.85\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-a5122f6d1bd6>:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_vlC-bdNzo1"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0FYVWkGOFBS",
    "outputId": "3bab1253-068e-41aa-a5af-34be9e25b98e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/59 [00:00<?, ?it/s]<ipython-input-4-a5122f6d1bd6>:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "loss=1.434532880783081 batch_id=58: 100%|██████████| 59/59 [00:31<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.5813, Accuracy: 4708/10000 (47%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "reIBU667OG_c",
    "outputId": "ce5f22e2-4d83-47eb-c53a-2e0605b09ef9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/59 [00:00<?, ?it/s]<ipython-input-4-a5122f6d1bd6>:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "loss=1.503180742263794 batch_id=58: 100%|██████████| 59/59 [00:28<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.4690, Accuracy: 3991/10000 (40%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6agTEkqzz6TZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
